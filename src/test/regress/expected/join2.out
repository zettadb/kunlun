begin;
BEGIN
set local min_parallel_table_scan_size = 0;
SET
set local parallel_setup_cost = 0;
SET
set local enable_hashjoin = on;
SET
-- Extract bucket and batch counts from an explain analyze plan.  In
-- general we can't make assertions about how many batches (or
-- buckets) will be required because it can vary, but we can in some
-- special cases and we can check for growth.
create or replace function find_hash(node json)
returns json language plpgsql
as
$$
declare
  x json;
  child json;
begin
  if node->>'Node Type' = 'Hash' then
    return node;
  else
    for child in select json_array_elements(node->'Plans')
    loop
      x = find_hash(child);
      if x is not null then
        return x;
      end if;
    end loop;
    return null;
  end if;
end;
$$;
CREATE FUNCTION
create or replace function hash_join_batches(query varchar(50))
returns table (original int, final int) language plpgsql
as
$$
declare
  whole_plan json;
  hash_node json;
begin
  for whole_plan in
    execute 'explain (analyze, format ''json'') ' || query
  loop
    hash_node = find_hash(json_extract_path(whole_plan, '0', 'Plan'));
    original = hash_node->>'Original Hash Batches';
    final = hash_node->>'Hash Batches';
    return next;
  end loop;
end;
$$;
CREATE FUNCTION
-- The "optimal" case: the hash table fits in memory; we plan for 1
-- batch, we stick to that number, and peak memory usage stays within
-- our work_mem budget
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
set local work_mem = '4MB';
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-oblivious hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '4MB';
SET
set local enable_parallel_hash = off;
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-aware hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '4MB';
SET
set local enable_parallel_hash = on;
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- The "good" case: batches required, but we plan the right number; we
-- plan for some number of batches, and we stick to that number, and
-- peak memory usage says within our work_mem budget
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
set local work_mem = '128kB';
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-oblivious hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '128kB';
SET
set local enable_parallel_hash = off;
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-aware hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '192kB';
SET
set local enable_parallel_hash = on;
SET
explain (costs off)
  select count(*) from simple r join simple s using (id);

select count(*) from simple r join simple s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- The "bad" case: during execution we need to increase number of
-- batches; in this case we plan for 1 batch, and increase at least a
-- couple of times, and peak memory usage stays within our work_mem
-- budget
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
set local work_mem = '128kB';
SET
explain (costs off)
  select count(*) FROM simple r JOIN bigger_than_it_looks s USING (id);

select count(*) FROM simple r JOIN bigger_than_it_looks s USING (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) FROM simple r JOIN bigger_than_it_looks s USING (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-oblivious hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '128kB';
SET
set local enable_parallel_hash = off;
SET
explain (costs off)
  select count(*) from simple r join bigger_than_it_looks s using (id);

select count(*) from simple r join bigger_than_it_looks s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join bigger_than_it_looks s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-aware hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 1;
SET
set local work_mem = '192kB';
SET
set local enable_parallel_hash = on;
SET
explain (costs off)
  select count(*) from simple r join bigger_than_it_looks s using (id);

select count(*) from simple r join bigger_than_it_looks s using (id);
 count 
-------
 20000
(1 row)

select original > 1 as initially_multibatch, final > original as increased_batches
  from hash_join_batches(
$$
  select count(*) from simple r join bigger_than_it_looks s using (id);
$$);
 initially_multibatch | increased_batches 
----------------------+-------------------
                      | 
(1 row)

rollback to settings;
ROLLBACK
-- The "ugly" case: increasing the number of batches during execution
-- doesn't help, so stop trying to fit in work_mem and hope for the
-- best; in this case we plan for 1 batch, increases just once and
-- then stop increasing because that didn't help at all, so we blow
-- right through the work_mem budget and hope for the best...
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
set local work_mem = '128kB';
SET
explain (costs off)
  select count(*) from simple r join extremely_skewed s using (id);

select count(*) from simple r join extremely_skewed s using (id);
 count 
-------
 20000
(1 row)

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id);
$$);
 original | final 
----------+-------
          |      
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-oblivious hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '128kB';
SET
set local enable_parallel_hash = off;
SET
explain (costs off)
  select count(*) from simple r join extremely_skewed s using (id);

select count(*) from simple r join extremely_skewed s using (id);
 count 
-------
 20000
(1 row)

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id);
$$);
 original | final 
----------+-------
          |      
(1 row)

rollback to settings;
ROLLBACK
-- parallel with parallel-aware hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 1;
SET
set local work_mem = '128kB';
SET
set local enable_parallel_hash = on;
SET
explain (costs off)
  select count(*) from simple r join extremely_skewed s using (id);

select count(*) from simple r join extremely_skewed s using (id);
 count 
-------
 20000
(1 row)

select * from hash_join_batches(
$$
  select count(*) from simple r join extremely_skewed s using (id);
$$);
 original | final 
----------+-------
          |      
(1 row)

rollback to settings;
ROLLBACK
-- A couple of other hash join tests unrelated to work_mem management.
-- Check that EXPLAIN ANALYZE has data even if the leader doesn't participate
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
set local work_mem = '4MB';
SET
set local parallel_leader_participation = off;
SET
select * from hash_join_batches(
$$
  select count(*) from simple r join simple s using (id);
$$);
 original | final 
----------+-------
          |      
(1 row)

rollback to settings;
ROLLBACK
-- Exercise rescans.  We'll turn off parallel_leader_participation so
-- that we can check that instrumentation comes back correctly.
drop table if exists join_foo;
psql:sql/join2.sql:258: NOTICE:  table "join_foo" does not exist, skipping
DROP TABLE
create table join_foo(id integer, t varchar(50));
CREATE TABLE
insert into join_foo select generate_series(1, 3) as id, 'xxxxx'::varchar(50) as t;
INSERT 0 3
--alter table join_foo set (parallel_workers = 0);
drop table if exists join_bar;
psql:sql/join2.sql:262: NOTICE:  table "join_bar" does not exist, skipping
DROP TABLE
create table join_bar(id integer, t varchar(50));
CREATE TABLE
insert into join_bar select generate_series(1, 10000) as id, 'xxxxx'::varchar(50) as t;
INSERT 0 10000
												  
--alter table join_bar set (parallel_workers = 2);
begin;
BEGIN
-- multi-batch with rescan, parallel-oblivious
savepoint settings;
SAVEPOINT
set enable_parallel_hash = off;
SET
set parallel_leader_participation = off;
SET
set min_parallel_table_scan_size = 0;
SET
set parallel_setup_cost = 0;
SET
set parallel_tuple_cost = 0;
SET
set max_parallel_workers_per_gather = 2;
SET
set enable_material = off;
SET
set enable_mergejoin = off;
SET
set work_mem = '64kB';
SET
explain (costs off)
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;

select count(*) from join_foo
  left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
  on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
 count 
-------
     3
(1 row)

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
$$);
 multibatch 
------------
 
(1 row)

rollback to settings;
ROLLBACK
-- single-batch with rescan, parallel-oblivious
savepoint settings;
SAVEPOINT
set enable_parallel_hash = off;
SET
set parallel_leader_participation = off;
SET
set min_parallel_table_scan_size = 0;
SET
set parallel_setup_cost = 0;
SET
set parallel_tuple_cost = 0;
SET
set max_parallel_workers_per_gather = 2;
SET
set enable_material = off;
SET
set enable_mergejoin = off;
SET
set work_mem = '4MB';
SET
explain (costs off)
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;

-- (crash or timeout) 
select count(*) from join_foo
  left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
  on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
 count 
-------
     3
(1 row)

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
$$);
 multibatch 
------------
 
(1 row)

rollback to settings;
ROLLBACK
-- multi-batch with rescan, parallel-aware
savepoint settings;
SAVEPOINT
set enable_parallel_hash = on;
SET
set parallel_leader_participation = off;
SET
set min_parallel_table_scan_size = 0;
SET
set parallel_setup_cost = 0;
SET
set parallel_tuple_cost = 0;
SET
set max_parallel_workers_per_gather = 2;
SET
set enable_material = off;
SET
set enable_mergejoin = off;
SET
set work_mem = '64kB';
SET
explain (costs off)
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;

select count(*) from join_foo
  left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
  on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
 count 
-------
     3
(1 row)

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
$$);
 multibatch 
------------
 
(1 row)

rollback to settings;
ROLLBACK
-- single-batch with rescan, parallel-aware
savepoint settings;
SAVEPOINT
set enable_parallel_hash = on;
SET
set parallel_leader_participation = off;
SET
set min_parallel_table_scan_size = 0;
SET
set parallel_setup_cost = 0;
SET
set parallel_tuple_cost = 0;
SET
set max_parallel_workers_per_gather = 2;
SET
set enable_material = off;
SET
set enable_mergejoin = off;
SET
set work_mem = '4MB';
SET
explain (costs off)
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;

select count(*) from join_foo
  left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
  on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
 count 
-------
     3
(1 row)

select final > 1 as multibatch
  from hash_join_batches(
$$
  select count(*) from join_foo
    left join (select b1.id, b1.t from join_bar b1 join join_bar b2 using (id)) ss
    on join_foo.id < ss.id + 1 and join_foo.id > ss.id - 1;
$$);
 multibatch 
------------
 
(1 row)

rollback to settings;
ROLLBACK
-- A full outer join where every record is matched.
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
explain (costs off)
     select  count(*) from simple r full outer join simple s using (id);

select  count(*) from simple r full outer join simple s using (id);
 count 
-------
 20000
(1 row)

rollback to settings;
ROLLBACK
-- parallelism not possible with parallel-oblivious outer hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
explain (costs off)
     select  count(*) from simple r full outer join simple s using (id);

select  count(*) from simple r full outer join simple s using (id);
 count 
-------
 20000
(1 row)

rollback to settings;
ROLLBACK
-- An full outer join where every record is not matched.
-- non-parallel
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 0;
SET
explain (costs off)
     select  count(*) from simple r full outer join simple s on (r.id = 0 - s.id);

select  count(*) from simple r full outer join simple s on (r.id = 0 - s.id);
 count 
-------
 40000
(1 row)

rollback to settings;
ROLLBACK
-- parallelism not possible with parallel-oblivious outer hash join
savepoint settings;
SAVEPOINT
set local max_parallel_workers_per_gather = 2;
SET
explain (costs off)
     select  count(*) from simple r full outer join simple s on (r.id = 0 - s.id);

select  count(*) from simple r full outer join simple s on (r.id = 0 - s.id);
 count 
-------
 40000
(1 row)

rollback to settings;
ROLLBACK
-- exercise special code paths for huge tuples (note use of non-strict
-- expression and left join required to get the detoasted tuple into
-- the hash table)
-- parallel with parallel-aware hash join (hits ExecParallelHashLoadTuple and
-- sts_puttuple oversized tuple cases because it's multi-batch)
savepoint settings;
SAVEPOINT
set max_parallel_workers_per_gather = 2;
SET
set enable_parallel_hash = on;
SET
set work_mem = '128kB';
SET
explain (costs off)
  select length(max(s.t))
  from wide left join (select id, coalesce(t, '') || '' as t from wide) s using (id);

select length(max(s.t))
from wide left join (select id, coalesce(t, '') || '' as t from wide) s using (id);
 length 
--------
 320000
(1 row)

select final > 1 as multibatch
  from hash_join_batches(
$$
  select length(max(s.t))
  from wide left join (select id, coalesce(t, '') || '' as t from wide) s using (id);
$$);
 multibatch 
------------
 
(1 row)

rollback to settings;
ROLLBACK
rollback;
ROLLBACK
reset enable_nestloop;
RESET
